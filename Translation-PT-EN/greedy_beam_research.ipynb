{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(1, os.path.abspath(os.path.join(os.path.dirname(f'..{os.sep}utils'))))\n",
    "sys.path.insert(1, os.path.abspath(os.path.join(os.path.dirname( '..'))))\n",
    "from utils.constants import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformer_v2 import Transformer\n",
    "from utils.function_utils import *\n",
    "from func_load_model_old import *\n",
    "from utils.optimizer_n_scheduler import *\n",
    "from utils.logging_tensorboard import create_summary_writer, log_loss, log_learning_rate, log_gradients, log_attention_weights\n",
    "from utils.distributions import *\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from data_funcs import *\n",
    "import numpy as np\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_TOKEN = \"<bos>\"\n",
    "EOS_TOKEN = \"<eos>\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"\"\"\n",
    "    Mask out subsequent positions.\n",
    "    \"\"\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_greedy_decode_v2(model, src, src_mask, max_len, tgt_tokenizer, tgt_vocab, tgt_pad_idx, batch_size, device):\n",
    "    batch_size = src.shape[0]\n",
    "    target_sentences_tokens = [[BOS_TOKEN] for _ in range(batch_size)]\n",
    "    trg_token_ids_batch = torch.tensor([[tgt_vocab[tokens[0]]] for tokens in target_sentences_tokens], device=device)\n",
    "    is_decoded = [False] * batch_size\n",
    "\n",
    "    tgt_itos = tgt_vocab.get_itos()\n",
    "    while True:\n",
    "        tgt_mask = generate_tgt_mask(trg_token_ids_batch, tgt_pad_idx)\n",
    "        enc_output, _ = model.encode(src, src_mask)\n",
    "        predicted_log_distributions, _, _ = model.decode(trg_token_ids_batch, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        num_of_trg_tokens = len(target_sentences_tokens[0])\n",
    "        predicted_log_distributions = predicted_log_distributions[num_of_trg_tokens - 1::num_of_trg_tokens]\n",
    "\n",
    "        most_probable_last_token_indices = torch.argmax(predicted_log_distributions, dim=-1).cpu().numpy()\n",
    "        most_probable_last_token_indices = most_probable_last_token_indices.reshape(-1)\n",
    "\n",
    "        predicted_words = [tgt_itos[index] for index in most_probable_last_token_indices.tolist()]\n",
    "\n",
    "        non_decoded_indices = [i for i, decoded in enumerate(is_decoded) if not decoded]\n",
    "\n",
    "        # Filter non_decoded_indices based on the length of predicted_words\n",
    "        non_decoded_indices = [i for i in non_decoded_indices if i < len(predicted_words)]\n",
    "\n",
    "        # Create a new list containing the words from predicted_words corresponding to non_decoded_indices\n",
    "        predicted_words_filtered = [predicted_words[idx] for idx in non_decoded_indices]\n",
    "\n",
    "        for non_decoded_idx, predicted_word in zip(non_decoded_indices, predicted_words_filtered):\n",
    "            target_sentences_tokens[non_decoded_idx].append(predicted_word)\n",
    "\n",
    "            if predicted_word == EOS_TOKEN:\n",
    "                is_decoded[non_decoded_idx] = True\n",
    "\n",
    "        if all(is_decoded) or num_of_trg_tokens == max_len:\n",
    "            break\n",
    "\n",
    "        # Filter out the decoded sentences and update the tensors accordingly\n",
    "        src = src[non_decoded_indices]\n",
    "        src_mask = src_mask[non_decoded_indices]\n",
    "        trg_token_ids_batch = trg_token_ids_batch[non_decoded_indices]\n",
    "        if len(non_decoded_indices) == 0:\n",
    "            break\n",
    "\n",
    "        most_probable_last_token_indices_filtered = torch.tensor([tgt_vocab[predicted_words_filtered[idx]] for idx, _ in enumerate(non_decoded_indices)], device=device)\n",
    "\n",
    "        trg_token_ids_batch = torch.cat((trg_token_ids_batch, most_probable_last_token_indices_filtered.unsqueeze(1)), 1)\n",
    "\n",
    "    return target_sentences_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_greedy_decode_v1(model, src, src_mask, max_len, tgt_tokenizer, tgt_vocab, tgt_pad_idx, batch_size):\n",
    "    with torch.no_grad():\n",
    "        sos_token = tgt_vocab[BOS_TOKEN]\n",
    "        tgt_tokens = torch.full((batch_size, 1), sos_token, dtype=torch.long, device=src.device)\n",
    "\n",
    "        for _ in range(max_len - 1):\n",
    "            tgt_mask = generate_tgt_mask(tgt_tokens, tgt_pad_idx)\n",
    "            output_probs = model(src, tgt_tokens, src_mask, tgt_mask)\n",
    "            _, next_tokens = torch.max(output_probs, dim=-1)\n",
    "            tgt_tokens = torch.cat([tgt_tokens, next_tokens], dim=1)\n",
    "            if torch.all(next_tokens == tgt_vocab[EOS_TOKEN]):\n",
    "                break\n",
    "\n",
    "    # decoded_sentences = [tgt_tokenizer.decode(tokens) for tokens in tgt_tokens.tolist()]\n",
    "    return tgt_tokens.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_beam_search_decode(model, src, src_mask, max_len, src_tokenizer, tgt_tokenizer, beam_size, batch_size):\n",
    "   #TODO\n",
    "   pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 16\n",
    "max_len = MODEL_MAX_SEQ_LEN\n",
    "d_model = MODEL_DIM\n",
    "num_layers = MODEL_N_LAYERS\n",
    "num_heads = MODEL_N_HEADS\n",
    "dropout = MODEL_DROPOUT\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-4\n",
    "warmup_steps = 2000\n",
    "weight_decay = 1e-4\n",
    "VOCAB_SIZE = 64_000\n",
    "d_ff = MODEL_FF\n",
    "label_smoothing = MODEL_LABEL_SMOTHING\n",
    "FILE_PATH = 'data/en-pt.txt'\n",
    "NUM_PHRASES = 50_000\n",
    "\n",
    "n=1\n",
    "LOGGING_FILE = f'runs{os.sep}translation_experiment_{n}'\n",
    "\n",
    "CHECKPOINT_PATH = 'checkpoints/checkpoint_epoch_10_val_loss_3.4311.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader, pad_idx_src, pad_idx_tgt, src_vocab, tgt_vocab, src_tokenizer, tgt_tokenizer = load_data(FILE_PATH, language_direction = LanguageDirection.PT2EN.name, limit = NUM_PHRASES, batch_size = batch_size, max_len = max_len, return_tokenizers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(len(src_vocab),\n",
    "                    len(tgt_vocab), \n",
    "                    d_model, \n",
    "                    num_heads, \n",
    "                    num_layers, \n",
    "                    d_ff, \n",
    "                    dropout, \n",
    "                    max_len).to(device)\n",
    "optimizer, scheduler = create_optimizer_and_scheduler(model, d_model, warmup_steps, learning_rate, weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, scheduler, epoch = load_checkpoint(model, optimizer, scheduler, CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(model, data_loader, src_pad_idx, tgt_pad_idx, tokenizer, tgt_vocab, device):\n",
    "    model.eval()\n",
    "    bleu_scores = []\n",
    "    tgt_itos = tgt_vocab.get_itos()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, tgt in data_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            src_mask = generate_src_mask(src, src_pad_idx)\n",
    "            output = batch_greedy_decode_v2(model, src, src_mask, max_len, tokenizer, tgt_vocab, tgt_pad_idx, batch_size, device)\n",
    "            \n",
    "            hypothesis = [sent[1:-1] for sent in output]  # Remove BOS and EOS tokens\n",
    "        \n",
    "            tgt_token_lists = [[tgt_itos[token_idx] for token_idx in sent if token_idx not in (tgt_pad_idx, tgt_vocab[BOS_TOKEN], tgt_vocab[EOS_TOKEN])] for sent in tgt.cpu().numpy()]\n",
    "            \n",
    "            return hypothesis, tgt_token_lists\n",
    "            for hyp, ref in zip(hypothesis, tgt_token_lists):\n",
    "                bleu = calculate_bleu(ref, hyp)\n",
    "                bleu_scores.append(bleu)\n",
    "    \n",
    "    model.train()\n",
    "    return sum(bleu_scores) / len(bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_decode_output, output = evaluate_metrics(model, test_dataloader, pad_idx_src, pad_idx_tgt, tgt_tokenizer, tgt_vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['anos', 'anos', 'anos', 'anos', 'anos'],\n",
       " ['anos', 'nas', 'anos', 'anos', 'anos'],\n",
       " ['anos', 'anos', 'anos', 'anos', 'anos'],\n",
       " ['anos', 'nas', 'anos', 'anos', 'anos'],\n",
       " ['anos', 'anos', 'anos', 'anos', 'anos'],\n",
       " ['anos', 'nas', 'nas', 'anos', 'anos'],\n",
       " ['anos', 'anos', 'anos', 'anos'],\n",
       " ['anos', 'nas', 'anos', 'anos'],\n",
       " ['anos', 'anos', 'anos', 'anos'],\n",
       " ['anos', 'nas', 'anos', 'anos'],\n",
       " ['anos', 'anos', 'anos'],\n",
       " ['anos', 'nas', 'nas'],\n",
       " ['anos', 'anos'],\n",
       " ['anos', 'nas'],\n",
       " ['anos', 'anos'],\n",
       " ['anos']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_decode_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['alguns', 'perguntavam', 'de', 'onde', 'és'],\n",
       " ['casas', 'para', 'alugar', 'em', 'hk', 'hong', 'kong'],\n",
       " ['todos', 'os', 'hotéis', 'em', 'cidade', 'do', 'luxemburgo'],\n",
       " ['seligman',\n",
       "  'o',\n",
       "  'que',\n",
       "  'visitar',\n",
       "  'em',\n",
       "  'seligman',\n",
       "  'o',\n",
       "  'que',\n",
       "  'visitar',\n",
       "  'em',\n",
       "  'seligman'],\n",
       " ['recomendado',\n",
       "  'para',\n",
       "  'turismo',\n",
       "  'de',\n",
       "  'negóciosturismo',\n",
       "  'familiaramantes',\n",
       "  'de',\n",
       "  'arte',\n",
       "  'e',\n",
       "  'design'],\n",
       " ['exposição', 'de', 'stéphan', 'zannini', 'na', 'artscad', 'com'],\n",
       " ['os',\n",
       "  'preciosos',\n",
       "  'extratos',\n",
       "  'de',\n",
       "  'origem',\n",
       "  'vegetal',\n",
       "  'contidos',\n",
       "  'no',\n",
       "  'produto',\n",
       "  'desempenham',\n",
       "  'uma',\n",
       "  'ação',\n",
       "  'relaxante',\n",
       "  'sobre',\n",
       "  'a',\n",
       "  'pele',\n",
       "  'neutralizando',\n",
       "  'as',\n",
       "  'manchas',\n",
       "  'da',\n",
       "  'pele',\n",
       "  'de',\n",
       "  'forma',\n",
       "  'eficaz',\n",
       "  'e',\n",
       "  'duradoura',\n",
       "  'durante',\n",
       "  'todo',\n",
       "  'o',\n",
       "  'período',\n",
       "  'de',\n",
       "  'tratamento'],\n",
       " ['sistelsa',\n",
       "  'é',\n",
       "  'uma',\n",
       "  'empresa',\n",
       "  'dedicada',\n",
       "  'à',\n",
       "  'segurança',\n",
       "  'eletrônica',\n",
       "  'entre',\n",
       "  'os',\n",
       "  'sistemas',\n",
       "  'é',\n",
       "  'instalado',\n",
       "  'alarme',\n",
       "  'anti',\n",
       "  'roubo',\n",
       "  'alarmes',\n",
       "  'contra',\n",
       "  'incêndio',\n",
       "  'acesso',\n",
       "  'cercas',\n",
       "  'elétricas',\n",
       "  'câmeras',\n",
       "  'de',\n",
       "  'segurança',\n",
       "  'lâmpadas'],\n",
       " ['é',\n",
       "  'analisada',\n",
       "  'também',\n",
       "  'a',\n",
       "  'prática',\n",
       "  'da',\n",
       "  'alocação',\n",
       "  'negociada',\n",
       "  'como',\n",
       "  'estratégia',\n",
       "  'de',\n",
       "  'resolução',\n",
       "  'de',\n",
       "  'conflitos'],\n",
       " ['junto',\n",
       "  'com',\n",
       "  'o',\n",
       "  'descobrimento',\n",
       "  'do',\n",
       "  'pavimento',\n",
       "  'continua',\n",
       "  'também',\n",
       "  'a',\n",
       "  'abertura',\n",
       "  'extraordinária',\n",
       "  'da',\n",
       "  'porta',\n",
       "  'do',\n",
       "  'céu',\n",
       "  'os',\n",
       "  'sótãos',\n",
       "  'da',\n",
       "  'catedral',\n",
       "  'locais',\n",
       "  'nunca',\n",
       "  'abertos',\n",
       "  'antes',\n",
       "  'nos',\n",
       "  'quais',\n",
       "  'ninguém',\n",
       "  'pôde',\n",
       "  'entrar',\n",
       "  'por',\n",
       "  'séculos'],\n",
       " ['uma',\n",
       "  'das',\n",
       "  'razões',\n",
       "  'para',\n",
       "  'isso',\n",
       "  'é',\n",
       "  'o',\n",
       "  'baixo',\n",
       "  'nível',\n",
       "  'relativo',\n",
       "  'de',\n",
       "  'investimentos',\n",
       "  'requeridos',\n",
       "  'para',\n",
       "  'sua',\n",
       "  'implantação',\n",
       "  'em',\n",
       "  'comparação',\n",
       "  'com',\n",
       "  'qualquer',\n",
       "  'outra',\n",
       "  'indústria'],\n",
       " ['não', 'foi', 'exclusiva'],\n",
       " ['a',\n",
       "  'extensa',\n",
       "  'base',\n",
       "  'de',\n",
       "  'clientes',\n",
       "  'da',\n",
       "  'colfax',\n",
       "  'international',\n",
       "  'inclui',\n",
       "  'empresas',\n",
       "  'da',\n",
       "  'fortune',\n",
       "  'instituições',\n",
       "  'de',\n",
       "  'ensino',\n",
       "  'e',\n",
       "  'agências',\n",
       "  'governamentais'],\n",
       " ['todas',\n",
       "  'as',\n",
       "  'terças',\n",
       "  'feiras',\n",
       "  'quinta',\n",
       "  'feira',\n",
       "  'e',\n",
       "  'sexta',\n",
       "  'feira',\n",
       "  'ciclopasseggiata',\n",
       "  'na',\n",
       "  'murgia',\n",
       "  'dei',\n",
       "  'trulli',\n",
       "  'no',\n",
       "  'valle',\n",
       "  'd',\n",
       "  'itria',\n",
       "  'e',\n",
       "  'ao',\n",
       "  'longo',\n",
       "  'da',\n",
       "  'costa',\n",
       "  'adriática',\n",
       "  'em',\n",
       "  'colaboração',\n",
       "  'com',\n",
       "  'o',\n",
       "  'iat',\n",
       "  'informação',\n",
       "  'e',\n",
       "  'recepção',\n",
       "  'turística'],\n",
       " ['cremos',\n",
       "  'que',\n",
       "  'o',\n",
       "  'movimento',\n",
       "  'espírita',\n",
       "  'vem',\n",
       "  'realizando',\n",
       "  'o',\n",
       "  'que',\n",
       "  'pode'],\n",
       " ['eles', 'eram', 'assim']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
