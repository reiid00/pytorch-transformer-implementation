{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(1, os.path.abspath(os.path.join(os.path.dirname(f'..{os.sep}utils'))))\n",
    "sys.path.insert(1, os.path.abspath(os.path.join(os.path.dirname( '..'))))\n",
    "from utils.constants import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformer_v2 import Transformer\n",
    "from utils.function_utils import *\n",
    "from func_load_model_old import *\n",
    "from utils.optimizer_n_scheduler import *\n",
    "from utils.logging_tensorboard import create_summary_writer, log_loss, log_learning_rate, log_gradients, log_attention_weights\n",
    "from utils.distributions import *\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from data_funcs import *\n",
    "import numpy as np\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_TOKEN = \"<bos>\"\n",
    "EOS_TOKEN = \"<eos>\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"\"\"\n",
    "    Mask out subsequent positions.\n",
    "    \"\"\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_greedy_decode_v2(model, src, src_mask, max_len, tgt_tokenizer, tgt_vocab, tgt_pad_idx, batch_size, device):\n",
    "    target_sentences_tokens = [[BOS_TOKEN] for _ in range(batch_size)]\n",
    "    trg_token_ids_batch = torch.tensor([[tgt_vocab[tokens[0]]] for tokens in target_sentences_tokens], device=device)\n",
    "    is_decoded = [False] * batch_size\n",
    "\n",
    "    while True:\n",
    "        tgt_mask = generate_tgt_mask(trg_token_ids_batch,tgt_pad_idx)\n",
    "        enc_output, _ = model.encode(src, src_mask)\n",
    "        predicted_log_distributions, _, _ = model.decode(trg_token_ids_batch, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        num_of_trg_tokens = len(target_sentences_tokens[0])\n",
    "        predicted_log_distributions= predicted_log_distributions[num_of_trg_tokens-1::num_of_trg_tokens]\n",
    "        \n",
    "        most_probable_last_token_indices = torch.argmax(predicted_log_distributions, dim=-1).cpu()\n",
    "\n",
    "        print(most_probable_last_token_indices)\n",
    "        predicted_words = [tgt_vocab[index] for index in most_probable_last_token_indices]\n",
    "\n",
    "        for idx, predicted_word in enumerate(predicted_words):\n",
    "            target_sentences_tokens[idx].append(predicted_word)\n",
    "\n",
    "            if predicted_word == EOS_TOKEN:  # once we find EOS token for a particular sentence we flag it\n",
    "                is_decoded[idx] = True\n",
    "\n",
    "        if all(is_decoded) or num_of_trg_tokens == max_len:\n",
    "            break\n",
    "        \n",
    "        trg_token_ids_batch = torch.cat((trg_token_ids_batch, torch.unsqueeze(torch.tensor(most_probable_last_token_indices, device=device), 1)), 1)\n",
    "\n",
    "        target_sentences_tokens_post = []\n",
    "        for target_sentence_tokens in target_sentences_tokens:\n",
    "            try:\n",
    "                target_index = target_sentence_tokens.index(EOS_TOKEN) + 1\n",
    "            except:\n",
    "                target_index = None\n",
    "\n",
    "            target_sentence_tokens = target_sentence_tokens[:target_index]\n",
    "            target_sentences_tokens_post.append(target_sentence_tokens)\n",
    "\n",
    "        return target_sentences_tokens_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_greedy_decode_v1(model, src, src_mask, max_len, tgt_tokenizer, tgt_vocab, tgt_pad_idx, batch_size):\n",
    "    with torch.no_grad():\n",
    "        sos_token = tgt_vocab[BOS_TOKEN]\n",
    "        tgt_tokens = torch.full((batch_size, 1), sos_token, dtype=torch.long, device=src.device)\n",
    "\n",
    "        for _ in range(max_len - 1):\n",
    "            tgt_mask = generate_tgt_mask(tgt_tokens, tgt_pad_idx)\n",
    "            output_probs = model(src, tgt_tokens, src_mask, tgt_mask)\n",
    "            _, next_tokens = torch.max(output_probs, dim=-1)\n",
    "            tgt_tokens = torch.cat([tgt_tokens, next_tokens], dim=1)\n",
    "            if torch.all(next_tokens == tgt_vocab[EOS_TOKEN]):\n",
    "                break\n",
    "\n",
    "    # decoded_sentences = [tgt_tokenizer.decode(tokens) for tokens in tgt_tokens.tolist()]\n",
    "    return tgt_tokens.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_beam_search_decode(model, src, src_mask, max_len, src_tokenizer, tgt_tokenizer, beam_size, batch_size):\n",
    "    with torch.no_grad():\n",
    "        memory, _ = model.encode(src, src_mask)\n",
    "        start_token = tgt_tokenizer.encode(\"<sos>\")[0]\n",
    "        end_token = tgt_tokenizer.encode(\"<eos>\")[0]\n",
    "\n",
    "        decoded_batches = []\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            decoder_input = torch.tensor([start_token]).unsqueeze(0).to(src.device)\n",
    "            partial_sequences = [([], 0, decoder_input)]\n",
    "\n",
    "            for _ in range(max_len):\n",
    "                new_partial_sequences = []\n",
    "\n",
    "                for seq, score, tokens in partial_sequences:\n",
    "                    if tokens[0, -1] == end_token:\n",
    "                        new_partial_sequences.append((seq, score, tokens))\n",
    "                        continue\n",
    "\n",
    "                    tgt_mask = (subsequent_mask(tokens.size(-1)).type_as(src_mask)).repeat(1, 1, 1)\n",
    "                    output_probs, _, _, _ = model(src[b].unsqueeze(0), tokens, src_mask[b].unsqueeze(0), tgt_mask, return_attention=True)\n",
    "                    top_probs, top_indices = torch.topk(F.log_softmax(output_probs[:, -1], dim=-1), beam_size)\n",
    "\n",
    "                    for i in range(beam_size):\n",
    "                        next_token = top_indices[0, i].unsqueeze(0).unsqueeze(0)\n",
    "                        current_prob = top_probs[0, i].item()\n",
    "                        new_tokens = torch.cat([tokens, next_token], dim=1)\n",
    "                        new_partial_sequences.append((seq + [next_token], score + current_prob, new_tokens))\n",
    "\n",
    "                partial_sequences = heapq.nlargest(beam_size, new_partial_sequences, key=lambda x: x[1])\n",
    "\n",
    "            best_sequence = max(partial_sequences, key=lambda x: x[1])[0]\n",
    "            decoded_sentence = tgt_tokenizer.decode([token.item() for token in best_sequence])\n",
    "            decoded_batches.append(decoded_sentence)\n",
    "\n",
    "    return decoded_batches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 16\n",
    "max_len = MODEL_MAX_SEQ_LEN\n",
    "d_model = MODEL_DIM\n",
    "num_layers = MODEL_N_LAYERS\n",
    "num_heads = MODEL_N_HEADS\n",
    "dropout = MODEL_DROPOUT\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-4\n",
    "warmup_steps = 2000\n",
    "weight_decay = 1e-4\n",
    "VOCAB_SIZE = 64_000\n",
    "d_ff = MODEL_FF\n",
    "label_smoothing = MODEL_LABEL_SMOTHING\n",
    "FILE_PATH = 'data/en-pt.txt'\n",
    "NUM_PHRASES = 10_000\n",
    "\n",
    "n=1\n",
    "LOGGING_FILE = f'runs{os.sep}translation_experiment_{n}'\n",
    "\n",
    "CHECKPOINT_PATH = 'checkpoints/checkpoint_epoch_8_val_loss_4.7865.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader, pad_idx_src, pad_idx_tgt, src_vocab, tgt_vocab, src_tokenizer, tgt_tokenizer = load_data(FILE_PATH, language_direction = LanguageDirection.PT2EN.name, limit = NUM_PHRASES, batch_size = batch_size, max_len = max_len, return_tokenizers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(len(src_vocab),\n",
    "                    len(tgt_vocab), \n",
    "                    d_model, \n",
    "                    num_heads, \n",
    "                    num_layers, \n",
    "                    d_ff, \n",
    "                    dropout, \n",
    "                    max_len).to(device)\n",
    "optimizer, scheduler = create_optimizer_and_scheduler(model, d_model, warmup_steps, learning_rate, weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, scheduler, epoch = load_checkpoint(model, optimizer, scheduler, CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(model, data_loader, src_pad_idx, tgt_pad_idx, tokenizer, tgt_vocab, device):\n",
    "    model.eval()\n",
    "    bleu_scores = []\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in data_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            src_mask= generate_src_mask(src,src_pad_idx)\n",
    "            output = batch_greedy_decode_v2(model, src, src_mask, max_len, tokenizer, tgt_vocab, tgt_pad_idx, batch_size, device)\n",
    "            return output\n",
    "            hypothesis = [tgt_vocab[token] for token in output if token not in (tgt_pad_idx, tgt_vocab[BOS_TOKEN], tgt_vocab[EOS_TOKEN])]\n",
    "            reference = [tgt_vocab[token] for token in tgt if token not in (tgt_pad_idx, tgt_vocab[BOS_TOKEN], tgt_vocab[EOS_TOKEN])]\n",
    "            bleu = calculate_bleu(reference, hypothesis)\n",
    "            bleu_scores.append(bleu)\n",
    "    \n",
    "    model.train()\n",
    "    return sum(bleu_scores) / len(bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[23],\n",
      "        [23],\n",
      "        [23],\n",
      "        [23],\n",
      "        [23],\n",
      "        [23],\n",
      "        [23],\n",
      "        [23],\n",
      "        [23],\n",
      "        [23],\n",
      "        [23],\n",
      "        [23],\n",
      "        [23],\n",
      "        [23],\n",
      "        [23],\n",
      "        [23]])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__getitem__(): incompatible function arguments. The following argument types are supported:\n    1. (self: torchtext._torchtext.Vocab, arg0: str) -> int\n\nInvoked with: <torchtext._torchtext.Vocab object at 0x000001AAF64938F0>, tensor([23])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m greedy_decode_output \u001b[39m=\u001b[39m evaluate_metrics(model, test_dataloader, pad_idx_src, pad_idx_tgt, tgt_tokenizer, tgt_vocab, device)\n",
      "Cell \u001b[1;32mIn[22], line 8\u001b[0m, in \u001b[0;36mevaluate_metrics\u001b[1;34m(model, data_loader, src_pad_idx, tgt_pad_idx, tokenizer, tgt_vocab, device)\u001b[0m\n\u001b[0;32m      6\u001b[0m src, tgt \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39mto(device), tgt\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m src_mask\u001b[39m=\u001b[39m generate_src_mask(src,src_pad_idx)\n\u001b[1;32m----> 8\u001b[0m output \u001b[39m=\u001b[39m batch_greedy_decode_v2(model, src, src_mask, max_len, tokenizer, tgt_vocab, tgt_pad_idx, batch_size, device)\n\u001b[0;32m      9\u001b[0m \u001b[39mreturn\u001b[39;00m output\n\u001b[0;32m     10\u001b[0m hypothesis \u001b[39m=\u001b[39m [tgt_vocab[token] \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m output \u001b[39mif\u001b[39;00m token \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (tgt_pad_idx, tgt_vocab[BOS_TOKEN], tgt_vocab[EOS_TOKEN])]\n",
      "Cell \u001b[1;32mIn[34], line 17\u001b[0m, in \u001b[0;36mbatch_greedy_decode_v2\u001b[1;34m(model, src, src_mask, max_len, tgt_tokenizer, tgt_vocab, tgt_pad_idx, batch_size, device)\u001b[0m\n\u001b[0;32m     14\u001b[0m most_probable_last_token_indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(predicted_log_distributions, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcpu()\n\u001b[0;32m     16\u001b[0m \u001b[39mprint\u001b[39m(most_probable_last_token_indices)\n\u001b[1;32m---> 17\u001b[0m predicted_words \u001b[39m=\u001b[39m [tgt_vocab[index] \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m most_probable_last_token_indices]\n\u001b[0;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m idx, predicted_word \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(predicted_words):\n\u001b[0;32m     20\u001b[0m     target_sentences_tokens[idx]\u001b[39m.\u001b[39mappend(predicted_word)\n",
      "Cell \u001b[1;32mIn[34], line 17\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     14\u001b[0m most_probable_last_token_indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(predicted_log_distributions, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcpu()\n\u001b[0;32m     16\u001b[0m \u001b[39mprint\u001b[39m(most_probable_last_token_indices)\n\u001b[1;32m---> 17\u001b[0m predicted_words \u001b[39m=\u001b[39m [tgt_vocab[index] \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m most_probable_last_token_indices]\n\u001b[0;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m idx, predicted_word \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(predicted_words):\n\u001b[0;32m     20\u001b[0m     target_sentences_tokens[idx]\u001b[39m.\u001b[39mappend(predicted_word)\n",
      "File \u001b[1;32mc:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torchtext\\vocab\\vocab.py:65\u001b[0m, in \u001b[0;36mVocab.__getitem__\u001b[1;34m(self, token)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mexport\n\u001b[0;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, token: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[0;32m     58\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39m        token: The token used to lookup the corresponding index.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39m        The index corresponding to the associated token.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab[token]\n",
      "\u001b[1;31mTypeError\u001b[0m: __getitem__(): incompatible function arguments. The following argument types are supported:\n    1. (self: torchtext._torchtext.Vocab, arg0: str) -> int\n\nInvoked with: <torchtext._torchtext.Vocab object at 0x000001AAF64938F0>, tensor([23])"
     ]
    }
   ],
   "source": [
    "greedy_decode_output = evaluate_metrics(model, test_dataloader, pad_idx_src, pad_idx_tgt, tgt_tokenizer, tgt_vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
