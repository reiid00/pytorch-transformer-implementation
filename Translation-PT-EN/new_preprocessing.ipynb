{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from enum import Enum\n",
    "from torch.utils.data import Dataset\n",
    "import itertools\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from functools import partial\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'data/en-pt.txt'\n",
    "NUM_PHRASES = 1_000_000\n",
    "OUTPUT_FILE = 'data/en-pt_sentences.txt'\n",
    "TOKENIZER_DIR = 'tokenizer'\n",
    "VOCAB_SIZE = 64_000\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "BOS_TOKEN = \"<bos>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "UNK_TOKEN = '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        en, pt = self.data[idx]\n",
    "        return torch.tensor(en), torch.tensor(pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageDirection(Enum):\n",
    "    PT2EN = 0,\n",
    "    EN2PT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_path, language_direction, limit=1000000):\n",
    "    with open(dataset_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        if language_direction == LanguageDirection.PT2EN.name:\n",
    "            sentence_pairs = [tuple(reversed(line.strip().split(\"\\t\"))) for line in itertools.islice(file, limit)]\n",
    "        else:\n",
    "            sentence_pairs = [tuple(line.strip().split(\"\\t\")) for line in itertools.islice(file, limit)]\n",
    "    return sentence_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert the text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-záàâãéèêíïóôõöúçñ]+', ' ', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_direction = LanguageDirection.PT2EN.name\n",
    "\n",
    "sentence_pairs = load_dataset(FILE_PATH, language_direction, limit=NUM_PHRASES)\n",
    "preprocessed_pairs = [(preprocess_text(en), preprocess_text(pt)) for en, pt in sentence_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'pt_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m en_tokenizer \u001b[39m=\u001b[39m get_tokenizer(\u001b[39m\"\u001b[39;49m\u001b[39mspacy\u001b[39;49m\u001b[39m\"\u001b[39;49m, language\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt_core_web_sm\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m pt_tokenizer \u001b[39m=\u001b[39m get_tokenizer(\u001b[39m\"\u001b[39m\u001b[39mspacy\u001b[39m\u001b[39m\"\u001b[39m, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39men_core_news_sm\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torchtext\\data\\utils.py:94\u001b[0m, in \u001b[0;36mget_tokenizer\u001b[1;34m(tokenizer, language)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 94\u001b[0m     spacy \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39;49mload(language)\n\u001b[0;32m     95\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIOError\u001b[39;00m:\n\u001b[0;32m     96\u001b[0m     \u001b[39m# Model shortcuts no longer work in spaCy 3.0+, try using fullnames\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     \u001b[39m# List is from https://github.com/explosion/spaCy/blob/b903de3fcb56df2f7247e5b6cfa6b66f4ff02b62/spacy/errors.py#L789\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     OLD_MODEL_SHORTCUTS \u001b[39m=\u001b[39m (\n\u001b[0;32m     99\u001b[0m         spacy\u001b[39m.\u001b[39merrors\u001b[39m.\u001b[39mOLD_MODEL_SHORTCUTS \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(spacy\u001b[39m.\u001b[39merrors, \u001b[39m\"\u001b[39m\u001b[39mOLD_MODEL_SHORTCUTS\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m    100\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\spacy\\__init__.py:54\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[0;32m     31\u001b[0m     name: Union[\u001b[39mstr\u001b[39m, Path],\n\u001b[0;32m     32\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     config: Union[Dict[\u001b[39mstr\u001b[39m, Any], Config] \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     38\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Language:\n\u001b[0;32m     39\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[39m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m util\u001b[39m.\u001b[39;49mload_model(\n\u001b[0;32m     55\u001b[0m         name,\n\u001b[0;32m     56\u001b[0m         vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[0;32m     57\u001b[0m         disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[0;32m     58\u001b[0m         enable\u001b[39m=\u001b[39;49menable,\n\u001b[0;32m     59\u001b[0m         exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[0;32m     60\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m     61\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\spacy\\util.py:449\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    448\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE941\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname, full\u001b[39m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[39m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE050\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'pt_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "en_tokenizer = get_tokenizer(\"spacy\", language=\"pt_core_web_sm\")\n",
    "pt_tokenizer = get_tokenizer(\"spacy\", language=\"en_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_pairs = [(\n",
    "    en_tokenizer(en),\n",
    "    pt_tokenizer(pt)\n",
    ") for en, pt in preprocessed_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_special_tokens(tokens, bos_token, eos_token):\n",
    "    return [bos_token] + tokens + [eos_token]\n",
    "\n",
    "tokenized_pairs = [(\n",
    "    add_special_tokens(en_tokens, BOS_TOKEN, EOS_TOKEN),\n",
    "    add_special_tokens(pt_tokens, BOS_TOKEN, EOS_TOKEN)\n",
    ") for en_tokens, pt_tokens in tokenized_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "def yield_tokens(pairs, language):\n",
    "    for en_tokens, pt_tokens in pairs:\n",
    "        if language == \"en\":\n",
    "            yield en_tokens\n",
    "        else:\n",
    "            yield pt_tokens\n",
    "\n",
    "en_vocab = build_vocab_from_iterator(yield_tokens(tokenized_pairs, \"en\"), specials=[UNK_TOKEN, PAD_TOKEN, BOS_TOKEN, EOS_TOKEN])\n",
    "pt_vocab = build_vocab_from_iterator(yield_tokens(tokenized_pairs, \"pt\"), specials=[UNK_TOKEN, PAD_TOKEN, BOS_TOKEN, EOS_TOKEN])\n",
    "\n",
    "index_pairs = [(\n",
    "    [en_vocab[token] for token in en_tokens],\n",
    "    [pt_vocab[token] for token in pt_tokens]\n",
    ") for en_tokens, pt_tokens in tokenized_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padded_sequence(sequence, max_length, pad_idx):\n",
    "    return sequence + [pad_idx] * (max_length - len(sequence))\n",
    "\n",
    "max_length = 50\n",
    "pad_idx_en = en_vocab[PAD_TOKEN]\n",
    "pad_idx_pt = pt_vocab[PAD_TOKEN]\n",
    "\n",
    "padded_pairs = [(\n",
    "    padded_sequence(en_indices, max_length, pad_idx_en),\n",
    "    padded_sequence(pt_indices, max_length, pad_idx_pt)\n",
    ") for en_indices, pt_indices in index_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TranslationDataset(padded_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    src_tensors, tgt_tensors = zip(*batch)\n",
    "    src_tensors = pad_sequence(src_tensors, batch_first=True, padding_value=pad_idx_en)\n",
    "    tgt_tensors = pad_sequence(tgt_tensors, batch_first=True, padding_value=pad_idx_pt)\n",
    "    return src_tensors, tgt_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Calculate the sizes of the training and testing sets\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(dataset_size * 0.8)  # Use 80% of the dataset for training\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders for the training and testing sets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2,  4228,    21,  ...,     1,     1,     1],\n",
      "        [    2,   473,     4,  ...,     1,     1,     1],\n",
      "        [    2,    67,  1247,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    2, 14787,    58,  ...,     1,     1,     1],\n",
      "        [    2,    32,   802,  ...,     1,     1,     1],\n",
      "        [    2,   119,     6,  ...,     1,     1,     1]]) tensor([[     2,   6783,     10,  ...,      1,      1,      1],\n",
      "        [     2,   1527,    587,  ...,      1,      1,      1],\n",
      "        [     2,    848,      7,  ...,      1,      1,      1],\n",
      "        ...,\n",
      "        [     2,    131,      9,  ...,      1,      1,      1],\n",
      "        [     2,    174,    808,  ...,      1,      1,      1],\n",
      "        [     2, 117745,     37,  ...,      1,      1,      1]])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (src, tgt) in enumerate(train_dataloader):\n",
    "    print(src, tgt)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
