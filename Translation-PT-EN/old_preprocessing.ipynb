{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import itertools\n",
    "import re\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import os\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'data/en-pt.txt'\n",
    "NUM_PHRASES = 1_000_000\n",
    "OUTPUT_FILE = 'data/en-pt_sentences.txt'\n",
    "TOKENIZER_DIR = 'tokenizer'\n",
    "VOCAB_SIZE = 64_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_path, limit=1000000):\n",
    "    \"\"\"\n",
    "    Loads the dataset from the given path.\n",
    "\n",
    "    Args:\n",
    "        dataset_path: The path to the dataset file.\n",
    "        limit: The maximum number of sentence pairs to load (default: 1000000).\n",
    "\n",
    "    Returns:\n",
    "        A list of (source, target) sentence pairs.\n",
    "    \"\"\"\n",
    "    with open(dataset_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        sentence_pairs = [tuple(line.strip().split(\"\\t\")) for line in itertools.islice(file, limit)]\n",
    "\n",
    "    return sentence_pairs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD-PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the given text.\n",
    "\n",
    "    Args:\n",
    "        text: The input text.\n",
    "        remove_stopwords: Whether to remove stopwords from the text (default: True).\n",
    "\n",
    "    Returns:\n",
    "        A list of preprocessed words.\n",
    "    \"\"\"\n",
    "    # Convert the text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-záàâãéèêíïóôõöúçñ]+', ' ', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tokenizer(sentence_pairs,temp_sentences_path, vocab_size=VOCAB_SIZE, min_frequency=2, output_dir=TOKENIZER_DIR):\n",
    "    \"\"\"\n",
    "    Trains a tokenizer on the given sentence pairs.\n",
    "\n",
    "    Args:\n",
    "        sentence_pairs: The list of (source, target) sentence pairs.\n",
    "        vocab_size: The vocabulary size for the tokenizer.\n",
    "        min_frequency: The minimum frequency for a token to be included in the vocabulary.\n",
    "        output_dir: The directory to save the tokenizer files.\n",
    "    \"\"\"\n",
    "    # Save all sentences to a temporary file\n",
    "    with open(temp_sentences_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        for src, tgt in sentence_pairs:\n",
    "            file.write(src + \"\\n\")\n",
    "            file.write(tgt + \"\\n\")\n",
    "\n",
    "    # Train the tokenizer\n",
    "    tokenizer = ByteLevelBPETokenizer()\n",
    "    tokenizer.train(files=[temp_sentences_path], vocab_size=vocab_size, min_frequency=min_frequency, special_tokens=[\n",
    "        \"<s>\",\n",
    "        \"<pad>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "        \"<mask>\",\n",
    "    ])\n",
    "\n",
    "    # Save the tokenizer\n",
    "    tokenizer.save_model(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, sentence_pairs, tokenizer, max_length):\n",
    "        self.sentence_pairs = sentence_pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentence_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source, target = self.sentence_pairs[idx]\n",
    "        tokenized_source = self.tokenizer.encode(source).ids[:self.max_length]\n",
    "        tokenized_target = self.tokenizer.encode(target).ids[:self.max_length]\n",
    "        return tokenized_source, tokenized_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(tokenizer_dir=TOKENIZER_DIR):\n",
    "    \"\"\"\n",
    "    Loads a tokenizer from the specified directory.\n",
    "\n",
    "    Args:\n",
    "        tokenizer_dir: The directory containing the tokenizer files.\n",
    "\n",
    "    Returns:\n",
    "        A Tokenizer object.\n",
    "    \"\"\"\n",
    "    tokenizer = ByteLevelBPETokenizer(\n",
    "        f\"{tokenizer_dir}{os.sep}vocab.json\",\n",
    "        f\"{tokenizer_dir}{os.sep}merges.txt\",\n",
    "        add_prefix_space=True\n",
    "    )\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(sentence_pairs, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Preprocesses the sentence pairs by tokenizing and creating a TranslationDataset.\n",
    "\n",
    "    Args:\n",
    "        sentence_pairs: A list of (source, target) sentence pairs.\n",
    "        tokenizer: The tokenizer used for tokenization.\n",
    "        max_length: The maximum sequence length.\n",
    "\n",
    "    Returns:\n",
    "        A TranslationDataset object.\n",
    "    \"\"\"\n",
    "    dataset = TranslationDataset(sentence_pairs, tokenizer, max_length)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(dataset, batch_size,tokenizer, shuffle=True, num_workers=0):\n",
    "    \"\"\"\n",
    "    Creates a DataLoader for the given dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset: A PyTorch Dataset object.\n",
    "        batch_size: The batch size to use in the DataLoader.\n",
    "        shuffle: Whether to shuffle the dataset before creating the DataLoader.\n",
    "        num_workers: The number of worker processes to use for loading the data.\n",
    "\n",
    "    Returns:\n",
    "        A DataLoader object.\n",
    "    \"\"\"\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=partial(collate_fn, tokenizer=tokenizer),\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "def collate_fn(batch, tokenizer):\n",
    "        src_tensors, tgt_tensors = zip(*batch)\n",
    "        src_tensors = [torch.tensor(src) for src in src_tensors]\n",
    "        tgt_tensors = [torch.tensor(tgt) for tgt in tgt_tensors]\n",
    "        src_tensors = pad_sequence(src_tensors, batch_first=True, padding_value=tokenizer.token_to_id(\"<pad>\"))\n",
    "        tgt_tensors = pad_sequence(tgt_tensors, batch_first=True, padding_value=tokenizer.token_to_id(\"<pad>\"))\n",
    "        return src_tensors, tgt_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a smaller subset of the dataset\n",
    "sentence_pairs = load_dataset(FILE_PATH, limit=NUM_PHRASES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_pairs = [(preprocess_text(en), preprocess_text(pt)) for en, pt in sentence_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>  results for pain in midrand gauteng </s>\n",
      "<s>  resultados encontrados para pain em midrand gauteng </s>\n"
     ]
    }
   ],
   "source": [
    "for k, v in preprocessed_pairs:\n",
    "    print(k)\n",
    "    print(v)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and save the tokenizer\n",
    "train_tokenizer(preprocessed_pairs, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_pairs = [(tokenizer.encode(en).ids, tokenizer.encode(pt).ids) for en, pt in preprocessed_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(preprocessed_pairs)\n",
    "split_idx = int(len(preprocessed_pairs) * 0.9)  # 90% for training, 10% for validation\n",
    "train_sentence_pairs = preprocessed_pairs[:split_idx]\n",
    "val_sentence_pairs = preprocessed_pairs[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = preprocess_data(train_sentence_pairs, tokenizer, 5000)\n",
    "val_dataset = preprocess_data(val_sentence_pairs, tokenizer, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[225, 32, 87, 34, 225, 2246, 355, 2733, 298, 61819, 12512, 272, 87, 34] [225, 32, 87, 34, 225, 2334, 7353, 353, 2733, 337, 61819, 12512, 272, 87, 34]\n"
     ]
    }
   ],
   "source": [
    "for k,v in train_dataset:\n",
    "    print(k, v)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_dataloader(train_dataset, 1, tokenizer, shuffle=False, num_workers=0)\n",
    "val_dataloader = create_dataloader(val_dataset, 1, tokenizer, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  225,    45,    82,   552, 16383,   296, 12832,  3727, 10276, 11285,\n",
      "          7498,  4254,   638,  8645,   297,  2441, 23252,    30,   296, 10077,\n",
      "          9511,   539,  4489,  2313,   317,  8638,   310,    17,   265,  3088,\n",
      "         35836,   297,  3727, 36647, 61299,    31,   296, 10077,  3356,   383,\n",
      "         41298, 25165,   297, 12832,  3727, 10276, 35654,  1862,  8638,   310,\n",
      "            17,   265,  3088,  9157,   324,    16,  8336,    31, 36289,   324,\n",
      "          9009,  8428,   383,  1708,    17,   514, 56037, 13226, 11285, 22332,\n",
      "           638,  5698,  3793,    18]]) tensor([[  225,    50,   831,  1473,   416,  4322,   545, 24734,  1182, 49270,\n",
      "           282,  3892,   276, 15621,   354, 20581, 32263,    17,   475,   337,\n",
      "          2511, 15925,    30,  3191,   371, 56289,   282, 14780,   282,  3312,\n",
      "           600,   654,   337,  4246, 24000,   282,  3892,    31,  3191,   545,\n",
      "         26518,   282, 19925, 62654, 30287,   465, 14780,   282,  3312,   600,\n",
      "           654,   468,  3892,   337, 15587,    17,   493,    17, 31058, 49270,\n",
      "           282,  3892,   276, 15621,   276,    16,  4322,    31, 21477,   276,\n",
      "         11163,   282, 38820,   282, 24734,  1182, 49270,   282,  3892,   276,\n",
      "         15621,    18]])\n"
     ]
    }
   ],
   "source": [
    "for k,v in train_dataloader:\n",
    "    print(k, v)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW-PREPROCESSING: Using Spacy Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
