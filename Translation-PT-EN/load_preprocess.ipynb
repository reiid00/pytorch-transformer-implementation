{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import itertools\n",
    "import re\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import os\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'data/en-pt.txt'\n",
    "NUM_PHRASES = 1_000_000\n",
    "OUTPUT_FILE = 'data/en-pt_sentences.txt'\n",
    "TOKENIZER_DIR = 'tokenizer'\n",
    "VOCAB_SIZE = 64_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_path, limit=1000000):\n",
    "    \"\"\"\n",
    "    Loads the dataset from the given path.\n",
    "\n",
    "    Args:\n",
    "        dataset_path: The path to the dataset file.\n",
    "        limit: The maximum number of sentence pairs to load (default: 1000000).\n",
    "\n",
    "    Returns:\n",
    "        A list of (source, target) sentence pairs.\n",
    "    \"\"\"\n",
    "    with open(dataset_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        sentence_pairs = [tuple(line.strip().split(\"\\t\")) for line in itertools.islice(file, limit)]\n",
    "\n",
    "    return sentence_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the given text.\n",
    "\n",
    "    Args:\n",
    "        text: The input text.\n",
    "        remove_stopwords: Whether to remove stopwords from the text (default: True).\n",
    "\n",
    "    Returns:\n",
    "        A list of preprocessed words.\n",
    "    \"\"\"\n",
    "    # Convert the text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-záàâãéèêíïóôõöúçñ]+', ' ', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tokenizer(sentence_pairs,temp_sentences_path, vocab_size=VOCAB_SIZE, min_frequency=2, output_dir=TOKENIZER_DIR):\n",
    "    \"\"\"\n",
    "    Trains a tokenizer on the given sentence pairs.\n",
    "\n",
    "    Args:\n",
    "        sentence_pairs: The list of (source, target) sentence pairs.\n",
    "        vocab_size: The vocabulary size for the tokenizer.\n",
    "        min_frequency: The minimum frequency for a token to be included in the vocabulary.\n",
    "        output_dir: The directory to save the tokenizer files.\n",
    "    \"\"\"\n",
    "    # Save all sentences to a temporary file\n",
    "    with open(temp_sentences_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        for src, tgt in sentence_pairs:\n",
    "            file.write(src + \"\\n\")\n",
    "            file.write(tgt + \"\\n\")\n",
    "\n",
    "    # Train the tokenizer\n",
    "    tokenizer = ByteLevelBPETokenizer()\n",
    "    tokenizer.train(files=[temp_sentences_path], vocab_size=vocab_size, min_frequency=min_frequency, special_tokens=[\n",
    "        \"<s>\",\n",
    "        \"<pad>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "        \"<mask>\",\n",
    "    ])\n",
    "\n",
    "    # Save the tokenizer\n",
    "    tokenizer.save_model(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, sentence_pairs, tokenizer, max_length):\n",
    "        self.sentence_pairs = sentence_pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentence_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source, target = self.sentence_pairs[idx]\n",
    "        tokenized_source = self.tokenizer.encode(source).ids[:self.max_length]\n",
    "        tokenized_target = self.tokenizer.encode(target).ids[:self.max_length]\n",
    "        return tokenized_source, tokenized_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(tokenizer_dir=TOKENIZER_DIR):\n",
    "    \"\"\"\n",
    "    Loads a tokenizer from the specified directory.\n",
    "\n",
    "    Args:\n",
    "        tokenizer_dir: The directory containing the tokenizer files.\n",
    "\n",
    "    Returns:\n",
    "        A Tokenizer object.\n",
    "    \"\"\"\n",
    "    tokenizer = ByteLevelBPETokenizer(\n",
    "        f\"{tokenizer_dir}{os.sep}vocab.json\",\n",
    "        f\"{tokenizer_dir}{os.sep}merges.txt\",\n",
    "        add_prefix_space=True\n",
    "    )\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(sentence_pairs, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Preprocesses the sentence pairs by tokenizing and creating a TranslationDataset.\n",
    "\n",
    "    Args:\n",
    "        sentence_pairs: A list of (source, target) sentence pairs.\n",
    "        tokenizer: The tokenizer used for tokenization.\n",
    "        max_length: The maximum sequence length.\n",
    "\n",
    "    Returns:\n",
    "        A TranslationDataset object.\n",
    "    \"\"\"\n",
    "    dataset = TranslationDataset(sentence_pairs, tokenizer, max_length)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(dataset, batch_size,tokenizer, shuffle=True, num_workers=0):\n",
    "    \"\"\"\n",
    "    Creates a DataLoader for the given dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset: A PyTorch Dataset object.\n",
    "        batch_size: The batch size to use in the DataLoader.\n",
    "        shuffle: Whether to shuffle the dataset before creating the DataLoader.\n",
    "        num_workers: The number of worker processes to use for loading the data.\n",
    "\n",
    "    Returns:\n",
    "        A DataLoader object.\n",
    "    \"\"\"\n",
    "    def collate_fn(batch):\n",
    "        src_tensors, tgt_tensors = zip(*batch)\n",
    "        src_tensors = pad_sequence(src_tensors, batch_first=True, padding_value=tokenizer.token_to_id(\"<pad>\"))\n",
    "        tgt_tensors = pad_sequence(tgt_tensors, batch_first=True, padding_value=tokenizer.token_to_id(\"<pad>\"))\n",
    "        return src_tensors, tgt_tensors\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a smaller subset of the dataset\n",
    "sentence_pairs = load_dataset(FILE_PATH, limit=NUM_PHRASES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_pairs = [(preprocess_text(en), preprocess_text(pt)) for en, pt in sentence_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and save the tokenizer\n",
    "train_tokenizer(preprocessed_pairs, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_pairs = [(tokenizer.encode(en).ids, tokenizer.encode(pt).ids) for en, pt in preprocessed_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(sentence_pairs)\n",
    "split_idx = int(len(sentence_pairs) * 0.9)  # 90% for training, 10% for validation\n",
    "train_sentence_pairs = sentence_pairs[:split_idx]\n",
    "val_sentence_pairs = sentence_pairs[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = preprocess_data(train_sentence_pairs, tokenizer, 5000)\n",
    "val_dataset = preprocess_data(val_sentence_pairs, tokenizer, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_dataloader(train_dataset, 32, tokenizer, shuffle=True, num_workers=0)\n",
    "val_dataloader = create_dataloader(val_dataset, 32, tokenizer, shuffle=False, num_workers=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
